{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "의사결정나무란?  \n",
    "학습 데이터의 분석하여 데이터에 내재되어 있는 패턴을 통해, 한번에 하나씩의 설명변수를 사용하여 분류와 화귀가 가능한 규칙들의 집합을 생성하는 모델  \n",
    "개념적으로 질문을 던져서 대장을 좁혀나가는 스무고개 놀이와 비슷한 개념  \n",
    " - 목적 y와 자료 x에 따라 적절한 분리 기준과 정지규칙(트리의 깊이)을 지정하여 의사결정 나무를 생성  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "의사결정 나무의 장점  \n",
    " 이해하기 쉽고 적용하기 쉬움.  if then 규칙에 의해 표현하기 때문에 모델이 쉽게 이해가능.  \n",
    "중요한 변수 선택에 유용 하다(상단에서 사용된 설명 변수가 중요한 변수 즉, 데이터 변별력이 가장 높다.)    \n",
    "  \n",
    "데이터의 통계적 가정이 필요없음(예시 LDA - 데이터 정규분포 가정)  \n",
    "  \n",
    "의사결정과정에 대한 설명 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "의사결정나무의 특성  \n",
    " Root node(뿌리노드)에서 분기가 거듭될 수록 그에 해당하는 데이터의 개수는 줄어듦  \n",
    "   \n",
    "각 leaf/terminal node(끝노드)에 속하는 데이터의 수의 총합은 root node의 데이터 수와 일치함. 즉, leaf node 간 교집합이 없음. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "의사결정나무의 단점    \n",
    "\n",
    "좋은 모형을 만들기 위해 많은 데이터가 필요하다.  \n",
    "모형을 만드는데 상대적으로 시간이 많이 소요 (tree building)  \n",
    "데이터의 변화에 민감하다. 데이터에 따라 모델이 변화한다.  \n",
    " - 학습과 테스트 데이터의 도메인이 유사해야함.  \n",
    "선형 구조형 데이터 예측시 더 복잡해짐  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 모델합습 추론 순서  \n",
    "데이터: 다변량 변수의 사용  \n",
    "  \n",
    "모델학습(트리구조 이용)  \n",
    "한번에 설명 변수 하나씩 데이터를 선택  \n",
    "2개 혹은 그 이상의 부분집합으로 분할함.  \n",
    "데이터 순도가 균일해지는 방향으로 재귀적 분할  y면 y x면 x의 데이터들이 각각의 끝 노드에 모여 있어야함.\n",
    " - 재귀적 분할 종료 조건  \n",
    "    분류문제: 끝 노드에 비슷한 범주(클래스)를 갖고 있는 관측데이터끼리 있다면 끝\n",
    "    예측문제: 끝 노드에 비슷한 수치(연속된 값)을 갖고 있는 관측데이터끼리 있다면 끝\n",
    "      \n",
    "추론(판별)  \n",
    "분류: 끝 노드에서 가장 빈도가 높은 범주(y)에 새로운 데이터를 분류  \n",
    "회귀: 끝 노드의 종속변수의 평균을(y) 예측 값으로 반환  \n",
    "  \n",
    "결정트리는 어떻게 분류 할 것인지가 중요함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "의사결정나무는 2 가지임.  \n",
    "분류 나무: 범주형 자료; 분류 \n",
    "회귀 나무: 연속형 수치; 예측  \n",
    "  \n",
    "분할기준: 불손도/불확실성  \n",
    "지니 계수  \n",
    "엔트로피 지수    \n",
    "  \n",
    "재귀적 분할 알고리즘  \n",
    "CART - Gini Index 기반  \n",
    "C4.5 - Entropy 기반"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 의사결정 나무의 분할 기준 이해   \n",
    "의사결정나무 분기 시의 변수의 영역 분할기준 \n",
    "순도: 균질성 증가    \n",
    "불순도 혹은 불확실성: 감소  \n",
    "즉, 불순도가 낮아지는 방향으로 나누어야 함.  \n",
    "  \n",
    "엔트로피란  \n",
    "  \n",
    "  정로량(event)    하나의 이벤트에 대한 것\n",
    "  Base -2 logarithm: 정보 측정 단위가 비트 임을 의미  \n",
    "  어떤 벤트 x가 발생할 확률이 1/2 사건에 대한 정보량은 어떤가? p(x) = 1/2: I(x) = 1 bit  \n",
    "  항상 일어나는 사건은 p(x) = 1 즉 0 bit   \n",
    "  \n",
    "  어떤 놀라움에 대한 양  \n",
    "  정보량이 많으면 놀라움이 적음  \n",
    "  정보량이 적으면 놀라움이 많음.  \n",
    "\n",
    "\n",
    "앤트로피  for a random variable 확률 분포에 대한 이벤트에 대한 것   \n",
    "\n",
    "랜덤변수의 정보량을 수치화 한 것.  랜덤변수에 정보량이 얼마나 돠는가를 수치화 함.  \n",
    "랜덤변수 x의 값(k)는 어떤 확률분포를 따르며 사건 x=k를 표현할 수 있는 평균 정보의 양을 bit로 표현  \n",
    "  엔트로피가 낮다: 불확실성이 낮음  \n",
    "  앤트로피가 높다: 불확실성이 높다  \n",
    "     \n",
    "  \n",
    "앤트로피 - 한개 영역  \n",
    "Entropy(A): m개의 범쥬(class)가 속하는 전체 A영역에 대한 엔트로피  \n",
    "Pk A영역에 속하는 레코드 가운데 k범주에 속하는 레코드의 비율  class1 class2\n",
    "  \n",
    "  \n",
    "  \n",
    "엔트로피 - 두 개 이상의 영역  \n",
    "m개의 범주가 속하는 A의 분할 영역에 대한 엔트로피  \n",
    "Ri 분할 전 레코드 중ㅇ에서 분할 후 i 영역에 속하는 레코드의 비율   \n",
    "\n",
    "엔트로피의 감소  \n",
    "불확실성 감소 = 순도 증가  \n",
    "즉, 의사결정나무 모델은 분할 후가 분할 전보다 불확실성이 감소하므로 분할을 결정함.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지니계수  \n",
    "지니계수 - 데이터의 통계적 분산정도를 정량화해서 표현한 값  \n",
    "Gini Impurity 혹은 Gini's diversity index: \n",
    "    불순도를 수치화 - 의사결정나무에서 분할기준으로 사용됨. 범주형 변수에 사용됨.  \n",
    "Gini coefficient  \n",
    "    실수형 변수에 사용됨  \n",
    "    ex) 경제적 불평등 정도를 수치화 함.      \n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결정의사나무에서 사용하는 지니계수는?  \n",
    "데이터의 통계적 분산정도를 정량화해서 표현한 값.  \n",
    "  \n",
    "GI(A): m개의 범주가 속하는 A영역에 대한 지니계수   \n",
    "\n",
    "지니계수 - 한개 영역  \n",
    "pk: A영역에 속하는 레코드 가운데 k 범주에 속하는 레코드 비율  \n",
    "\n",
    "지니계수 - 두 개 이상의 영역  \n",
    "GI(A): m개의 범주가 속하는 A의 분할 영역에 대한 지니계수   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목표변수: 범주형 변수  \n",
    "  \n",
    "분류 알고리즘과 불순도 지표  \n",
    "CART: 지니계수  \n",
    "C4.5: 엔트로피, 정보이익, 정보이익비율  \n",
    "CHAID: 카이 제곱 통계량\n",
    "  \n",
    "분류 결과(판별, 추론)  \n",
    "소속집단 판단, 경향성도 확률로 표현가능 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류나무: CART  \n",
    "CART 알고리즘  \n",
    "가장 널리 사용하는 의사결정나무 알고리즘  \n",
    "분류 나무 ,회귀 나므  \n",
    "이진분할  \n",
    "Gini Index는 낮아지는 것이 좋음.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C4.5 알고리즘  \n",
    "분류나무, 회귀나무  \n",
    "다중분할  \n",
    "불순도 알조리즘: 엔트로피, 정보이익, 정보이득률  \n",
    "정보이익(IG) - 정보의 가치   가 높아져야 좋음  \n",
    "  \n",
    "분할 전과 분할 후의 차이가 높으면 좋음 즉 불확실성이 높게 감소한 것이 좋음.  \n",
    "  \n",
    "  \n",
    "정보이득율  \n",
    "C4.5에서는 정보 이득율을 추가적으로 도입  \n",
    "가지(branch)의 수가 많아질수록 정보이득이 높아지는 경향을 보임    \n",
    "그러나, 데이터를 너무 잘게 분해하는 것은 좋은 지표라고 볼 수 없으므로, 정보 이득에는 한계점이 있음.  \n",
    "이러한 단점을 보안하기 위해 IV 도입 정보 이득율을 정구화 하고자 함.  \n",
    "즉, 가지가 많으면 감점  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "의사결정 나무에서의 overfitting 문제 방지  \n",
    "  \n",
    "과접합을 피하는 방법 - 나무의 성장을 중단시킴.  \n",
    "  \n",
    "성장멈추기 stop condition  \n",
    "나무 모델의 깊이를 파라미터로 설정  \n",
    "나무 모델을 성장시키면서 특정 조건에서 성장을 중단  \n",
    "노드 내의 최소 관측치의 수    \n",
    "\n",
    "  \n",
    "불순도 최소 감소향  \n",
    "CHAID에서 사용  \n",
    "가지치기를 사용하지 않고 종료  \n",
    "    \n",
    "      \n",
    "\n",
    "가지치기(Pruning)  \n",
    "  \n",
    "완전모형 생성 후 가지치기 진행  \n",
    "데이터를 버리는 개념이 아닌 merge의 개념.    \n",
    "  \n",
    "pruning 단계  \n",
    "데이터를 training/pruning/test 데이터로 분류  \n",
    "학습데이터로 의사결정 나무 생성  \n",
    "    pruning data로 가지치기 수행  (가지치기 비용함수를 최소로 하는 분기를 찾음)  \n",
    "    Cost(T): 오류가 적으면서 terminal node 수가 적은 단순한 모델일 수록 작은 값  \n",
    "    Err(T): 검증 데이터에 대한 오분류율  \n",
    "    L(T): leaf node의 수  \n",
    "    알파: Err(T)와 L(T)를 결합하는 가중치 값 선택  \n",
    "가지치기가 끝난 의사결정나무에 시험데이터로 결과 예측  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 의사결정나무 - 회귀나무  \n",
    "목표변수: 수치형 변수  \n",
    "회귀결과(입력 데이터의 결과 예측)  \n",
    "블순도 측정 방법  SSE  \n",
    "오차 = 실제값 - 예측값  \n",
    "성능평가방법 \n",
    "RMSE  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 의사결정나무 - 앙상블  \n",
    "앙상블이란? 다수의 표보느 다수의 모델에서 나온 결과를 다수결로 결정하는 것을 말 함  \n",
    "  \n",
    "Random Forest  \n",
    "여러개의 weak learner을 결합하면 single learner보다 더 좋은 성능을 낼 수 있다는 가정하에 제안된 의사결정나무 기반의 학습모델  \n",
    "    sigle learner 전체 데이터를 학습한 하나의 decision tree를 의미  \n",
    "    swak learner 임의의 일부 데이터를 학습한 decision tree를 의미  \n",
    "  \n",
    "Bootstrap 사용   \n",
    " 전체 데이터로부터 무작위 복원 추출을 이용하여 여러 개의 학습 데이터 표본 샘플을 추출함.  (뽑고 다시 넣고 다시 랜덤하게 뽑음)  \n",
    "\n",
    "Forest 생성  \n",
    " 무작위로 예측 변수를 선택하여 모델 구축  \n",
    " 즉, 의사결정나무는 예측 변수 선택 시 지준 지표를 사용하였으나 무작위 숲에서는 무작위로 선택  \n",
    "\n",
    "\n",
    "\n",
    "앙상블 결과 결합(분류 문제) -> 투표, (예측문제) -> 평균    \n",
    "나무 구조에서 숲이 되면서 해석 가능한 모델의 장점이 사라짐.  \n",
    "그러나 결과 분석을 통해 설명 변수 중 중요한 변수의 판별이 가능함.   \n",
    "무작위 로 뽑아서 값을 살펴보게 됨.  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
