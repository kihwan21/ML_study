{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습된 퍼셉트론의 매개변수:  [[2. 2.]] [-1.]\n",
      "훈련집합에 대한 예측:  [-1  1  1  1]\n",
      "정확률 측정:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "#x y train셋\n",
    "X=[[0,0], [0,1], [1,0], [1,1]]\n",
    "y=[-1,1,1,1]\n",
    "\n",
    "#perceptron 학습\n",
    "p = Perceptron()\n",
    "p.fit(X, y)\n",
    "\n",
    "print(\"학습된 퍼셉트론의 매개변수: \", p.coef_, p.intercept_)\n",
    "print(\"훈련집합에 대한 예측: \", p.predict(X))\n",
    "print(\"정확률 측정: \", p.score(X,y)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#데이터셋 훈련과 테스트 집합으로 분할하기\n",
    "digit = datasets.load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digit.data, digit.target, train_size=0.6)\n",
    "\n",
    "#학습하기 \n",
    "p = Perceptron(max_iter=100, eta0=0.001, verbose=0)\n",
    "p.fit(X_train, y_train)  \n",
    "\n",
    "res=p.predict(X_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다중 퍼셉트론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.16368284\n",
      "Iteration 2, loss = 1.36301467\n",
      "Iteration 3, loss = 0.87464773\n",
      "Iteration 4, loss = 0.57805339\n",
      "Iteration 5, loss = 0.42736462\n",
      "Iteration 6, loss = 0.34026627\n",
      "Iteration 7, loss = 0.26416250\n",
      "Iteration 8, loss = 0.22248145\n",
      "Iteration 9, loss = 0.18307737\n",
      "Iteration 10, loss = 0.15786860\n",
      "Iteration 11, loss = 0.13809217\n",
      "Iteration 12, loss = 0.13649511\n",
      "Iteration 13, loss = 0.11084989\n",
      "Iteration 14, loss = 0.09834550\n",
      "Iteration 15, loss = 0.08772022\n",
      "Iteration 16, loss = 0.07844633\n",
      "Iteration 17, loss = 0.07213318\n",
      "Iteration 18, loss = 0.06586149\n",
      "Iteration 19, loss = 0.05850096\n",
      "Iteration 20, loss = 0.05777329\n",
      "Iteration 21, loss = 0.05205848\n",
      "Iteration 22, loss = 0.04701992\n",
      "Iteration 23, loss = 0.04419600\n",
      "Iteration 24, loss = 0.04219781\n",
      "Iteration 25, loss = 0.03928926\n",
      "Iteration 26, loss = 0.03482210\n",
      "Iteration 27, loss = 0.03354785\n",
      "Iteration 28, loss = 0.03087007\n",
      "Iteration 29, loss = 0.03080851\n",
      "Iteration 30, loss = 0.02790690\n",
      "Iteration 31, loss = 0.02504641\n",
      "Iteration 32, loss = 0.02443385\n",
      "Iteration 33, loss = 0.02393032\n",
      "Iteration 34, loss = 0.02193236\n",
      "Iteration 35, loss = 0.02051835\n",
      "Iteration 36, loss = 0.01887735\n",
      "Iteration 37, loss = 0.01871206\n",
      "Iteration 38, loss = 0.01758973\n",
      "Iteration 39, loss = 0.01631648\n",
      "Iteration 40, loss = 0.01665768\n",
      "Iteration 41, loss = 0.01538042\n",
      "Iteration 42, loss = 0.01481351\n",
      "Iteration 43, loss = 0.01441619\n",
      "Iteration 44, loss = 0.01359256\n",
      "Iteration 45, loss = 0.01433833\n",
      "Iteration 46, loss = 0.01263368\n",
      "Iteration 47, loss = 0.01214829\n",
      "Iteration 48, loss = 0.01243070\n",
      "Iteration 49, loss = 0.01146734\n",
      "Iteration 50, loss = 0.01116069\n",
      "Iteration 51, loss = 0.01055646\n",
      "Iteration 52, loss = 0.01029610\n",
      "Iteration 53, loss = 0.00967098\n",
      "Iteration 54, loss = 0.00960948\n",
      "Iteration 55, loss = 0.00939102\n",
      "Iteration 56, loss = 0.00899190\n",
      "Iteration 57, loss = 0.00899640\n",
      "Iteration 58, loss = 0.00857773\n",
      "Iteration 59, loss = 0.00832118\n",
      "Iteration 60, loss = 0.00828483\n",
      "Iteration 61, loss = 0.00834646\n",
      "Iteration 62, loss = 0.00766560\n",
      "Iteration 63, loss = 0.00752934\n",
      "Iteration 64, loss = 0.00736510\n",
      "Iteration 65, loss = 0.00753773\n",
      "Iteration 66, loss = 0.00695267\n",
      "Iteration 67, loss = 0.00679692\n",
      "Iteration 68, loss = 0.00665653\n",
      "Iteration 69, loss = 0.00661680\n",
      "Iteration 70, loss = 0.00638612\n",
      "Iteration 71, loss = 0.00628536\n",
      "Iteration 72, loss = 0.00609704\n",
      "Iteration 73, loss = 0.00599941\n",
      "Iteration 74, loss = 0.00603388\n",
      "Iteration 75, loss = 0.00569420\n",
      "Iteration 76, loss = 0.00574286\n",
      "Iteration 77, loss = 0.00567108\n",
      "Iteration 78, loss = 0.00542006\n",
      "Iteration 79, loss = 0.00540608\n",
      "Iteration 80, loss = 0.00542619\n",
      "Iteration 81, loss = 0.00511466\n",
      "Iteration 82, loss = 0.00510139\n",
      "Iteration 83, loss = 0.00497683\n",
      "Iteration 84, loss = 0.00487381\n",
      "Iteration 85, loss = 0.00486440\n",
      "Iteration 86, loss = 0.00479043\n",
      "Iteration 87, loss = 0.00467902\n",
      "Iteration 88, loss = 0.00458937\n",
      "Iteration 89, loss = 0.00464695\n",
      "Iteration 90, loss = 0.00451801\n",
      "Iteration 91, loss = 0.00438160\n",
      "Iteration 92, loss = 0.00440031\n",
      "Iteration 93, loss = 0.00426499\n",
      "Iteration 94, loss = 0.00415274\n",
      "Iteration 95, loss = 0.00413115\n",
      "Iteration 96, loss = 0.00401631\n",
      "Iteration 97, loss = 0.00402994\n",
      "Iteration 98, loss = 0.00398961\n",
      "Iteration 99, loss = 0.00387366\n",
      "Iteration 100, loss = 0.00386819\n",
      "Iteration 101, loss = 0.00379505\n",
      "Iteration 102, loss = 0.00377243\n",
      "Iteration 103, loss = 0.00376051\n",
      "Iteration 104, loss = 0.00368387\n",
      "Iteration 105, loss = 0.00363279\n",
      "Iteration 106, loss = 0.00357415\n",
      "Iteration 107, loss = 0.00360825\n",
      "Iteration 108, loss = 0.00346831\n",
      "Iteration 109, loss = 0.00340753\n",
      "Iteration 110, loss = 0.00338284\n",
      "Iteration 111, loss = 0.00336408\n",
      "Iteration 112, loss = 0.00328818\n",
      "Iteration 113, loss = 0.00329067\n",
      "Iteration 114, loss = 0.00329017\n",
      "Iteration 115, loss = 0.00320184\n",
      "Iteration 116, loss = 0.00315810\n",
      "Iteration 117, loss = 0.00315619\n",
      "Iteration 118, loss = 0.00308420\n",
      "Iteration 119, loss = 0.00307573\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=50, hidden_layer_sizes=(100, 50, 100, 30),\n",
       "              max_iter=300, solver='sgd', verbose=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "#데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
    "digit=datasets.load_digits()\n",
    "x_train,x_test,y_train,y_test=train_test_split(digit.data, digit.target, train_size=0.6)\n",
    "\n",
    "#MLP 분류기 모델 학습 각 콤마마다 층을 추가 할 수 있음 \n",
    "#은닉층은 4개고 각 은닉층에 데이터셋은 100개.learning_rate_init 학습률 배치 사이즈=32 즉, 한번에 학습 할때 저만큼 쪼개서 하겠다. max_liter 최대 300번 학습. solver 경사하강법중 sgd로 사용\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(100,50,100,30),learning_rate_init=0.001,batch_size=50,max_iter=300,solver='sgd',verbose=True)\n",
    "mlp.fit(x_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x 예측값:  [7 1 1 ... 0 2 8]\n",
      "y 예측값:  [9 2 9 7 3 2 8 3 6 1 6 2 0 9 8 0 0 3 0 3 9 4 9 9 3 8 4 6 7 5 1 9 3 7 9 8 2\n",
      " 2 2 5 4 1 9 5 9 9 5 4 4 4 3 8 5 3 1 3 4 2 4 1 2 7 4 6 8 4 3 4 0 5 4 4 8 0\n",
      " 7 0 0 4 6 1 2 9 8 0 5 9 3 5 4 3 2 2 3 3 8 3 1 4 0 9 1 0 1 6 5 3 1 0 5 7 8\n",
      " 8 8 5 0 2 8 6 4 3 9 2 7 8 8 7 7 7 2 0 8 8 4 9 0 8 5 3 2 2 8 5 2 5 3 6 5 8\n",
      " 2 3 2 4 5 1 4 4 1 1 3 5 1 4 6 5 2 1 8 4 8 6 4 0 8 6 6 3 8 1 7 3 5 0 4 7 8\n",
      " 4 9 3 1 3 2 5 1 4 5 1 4 7 7 8 6 1 3 2 5 7 9 3 0 2 7 3 3 4 6 6 0 2 7 7 1 3\n",
      " 6 2 6 1 6 9 8 9 9 5 8 1 5 1 6 0 6 0 5 8 4 2 9 7 7 4 6 7 4 3 6 7 5 4 5 0 7\n",
      " 9 8 5 7 2 6 4 7 2 7 2 9 3 2 2 4 0 2 8 0 1 5 5 6 2 7 9 9 5 1 7 2 0 8 4 7 3\n",
      " 2 1 3 7 2 0 9 1 2 2 4 7 8 4 5 7 2 4 6 0 1 5 1 8 3 0 3 6 2 8 1 1 0 3 4 5 9\n",
      " 3 9 2 5 5 5 7 3 2 9 7 5 3 7 3 2 0 9 2 2 5 6 9 4 2 0 4 3 8 0 1 6 3 1 6 0 5\n",
      " 2 6 2 2 1 9 0 5 7 0 8 0 7 7 8 2 3 5 5 1 4 8 7 3 2 5 9 0 5 1 4 7 8 5 5 1 6\n",
      " 1 1 7 0 6 6 8 5 7 0 4 0 7 9 5 1 0 6 6 7 1 4 1 6 5 7 2 5 8 2 2 4 8 9 3 8 8\n",
      " 0 1 0 1 3 5 2 5 7 9 4 1 1 0 0 2 9 9 9 7 6 8 8 1 8 1 7 2 8 0 6 3 7 5 0 3 5\n",
      " 8 1 3 3 1 5 3 9 1 9 3 6 7 4 2 5 8 1 2 9 8 9 4 4 5 6 1 7 1 0 0 6 4 8 4 3 4\n",
      " 3 3 6 1 3 9 4 5 5 5 9 2 6 6 7 2 8 6 9 5 2 7 2 5 4 0 6 6 7 0 5 2 2 8 1 1 9\n",
      " 7 3 0 8 0 3 6 7 6 9 1 0 3 7 4 8 0 2 3 1 4 5 4 6 3 9 6 9 6 5 2 0 8 7 0 8 9\n",
      " 3 8 2 9 3 2 8 4 5 7 1 3 7 4 4 4 0 2 4 5 0 7 1 7 4 0 8 1 6 7 0 3 7 3 8 4 1\n",
      " 0 9 9 9 1 8 6 8 6 4 6 8 4 3 6 8 4 3 7 3 9 7 4 3 5 1 5 4 0 5 4 4 0 5 6 1 7\n",
      " 1 3 2 6 9 8 3 6 9 8 8 6 4 9 9 9 8 0 6 4 5 8 8 1 4 4 1 7 7 1 6 4 6 3 3 1 4\n",
      " 3 4 7 7 6 9 3 7 4 2 5 1 3 3 1 6]\n"
     ]
    }
   ],
   "source": [
    "X_train_pred=mlp.predict(x_train)\n",
    "y_train_pred=mlp.predict(x_test)\n",
    "\n",
    "print(\"x 예측값: \", X_train_pred)\n",
    "print(\"y 예측값: \",y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf=np.zeros((10,10))\n",
    "# for i in range(len(res)):\n",
    "#     conf[res[i]][y_test[i]]+=1\n",
    "# print(conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_correct=0\n",
    "# for i in range(10):\n",
    "#     no_correct += conf[i][i]\n",
    "# accuracy=no_correct/len(res)\n",
    "# print(\"테스트 집합 정확률\", accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.30269763\n",
      "Iteration 2, loss = 0.12153856\n",
      "Iteration 3, loss = 0.08965296\n",
      "Iteration 4, loss = 0.06550131\n",
      "Iteration 5, loss = 0.05136353\n",
      "Iteration 6, loss = 0.04285980\n",
      "Iteration 7, loss = 0.03507026\n",
      "Iteration 8, loss = 0.03215136\n",
      "Iteration 9, loss = 0.03201114\n",
      "Iteration 10, loss = 0.02199350\n",
      "Iteration 11, loss = 0.01880433\n",
      "Iteration 12, loss = 0.01746283\n",
      "Iteration 13, loss = 0.02542430\n",
      "Iteration 14, loss = 0.01782710\n",
      "Iteration 15, loss = 0.01978683\n",
      "Iteration 16, loss = 0.02094735\n",
      "Iteration 17, loss = 0.02425677\n",
      "Iteration 18, loss = 0.01602768\n",
      "Iteration 19, loss = 0.01750498\n",
      "Iteration 20, loss = 0.01125328\n",
      "Iteration 21, loss = 0.01578583\n",
      "Iteration 22, loss = 0.01969069\n",
      "Iteration 23, loss = 0.02024840\n",
      "Iteration 24, loss = 0.02244460\n",
      "Iteration 25, loss = 0.01764443\n",
      "Iteration 26, loss = 0.01277120\n",
      "Iteration 27, loss = 0.01739834\n",
      "Iteration 28, loss = 0.00904572\n",
      "Iteration 29, loss = 0.01958375\n",
      "Iteration 30, loss = 0.02580132\n",
      "Iteration 31, loss = 0.01683861\n",
      "Iteration 32, loss = 0.01864193\n",
      "Iteration 33, loss = 0.01737748\n",
      "Iteration 34, loss = 0.01506902\n",
      "Iteration 35, loss = 0.01428925\n",
      "Iteration 36, loss = 0.01380516\n",
      "Iteration 37, loss = 0.01881778\n",
      "Iteration 38, loss = 0.01658545\n",
      "Iteration 39, loss = 0.01357437\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier    # 다중 레이블 분류\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "# MNIST 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
    "mnist = fetch_openml('mnist_784')\n",
    "mnist.data=mnist.data/255.0 #[0,255] 범위를 [0,1]범위로 변환\n",
    "\n",
    "# x\n",
    "x_train=mnist.data[:60000]\n",
    "x_test=mnist.data[60000:]\n",
    "\n",
    "# y\n",
    "y_train=np.int16(mnist.target[:60000])\n",
    "y_test=np.int16(mnist.target[600000:])\n",
    "\n",
    "#MLP 분류기 모델을 학습\n",
    "#은닉층은 1개고 각 은닉층에 데이터셋은 100개.learning_rate_init 학습률 / batch_size 즉, 한번에 학습 할때 저만큼 쪼개서 하겠다. max_liter 최대 300번 학습. solver 경사하강법중 adam 사용\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100), learning_rate_init=0.01, batch_size=512, max_iter=300, solver='adam', verbose=True)\n",
    "mlp.fit(x_train, y_train)\n",
    "\n",
    "#테스트 집합으로 예측 \n",
    "res = mlp.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 단일 하이퍼 매개변수 최적화: validation_curve 함수 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import validation_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# 데이터셋을 읽고 휸련 집합과 테스트 집합으로 분할 \n",
    "digit = datasets.load_digits()\n",
    "x_train,x_test,y_train,y_test = train_test_split(digit.data, digit.target, train_size=0.6)\n",
    "\n",
    "#다층 퍼셉트론을 교차 검증으로 성능평가\n",
    "start = time.time()\n",
    "mlp = MLPClassifier(learning_rate_init=0.001, batch_size=32, max_iter=300, solver='sgd')\n",
    "prange=range(40, 1001, 50)  # 50에서 시작하여 50씩 증가시키면서 1000까지 조사\n",
    "\n",
    "train_score,test_score=validation_curve(mlp,x_train,y_train,param_name=\"hidden_layer_sizes\", param_range=prange, cv=10, scoring=\"accuracy\", n_jobs=4) #cv=10 10겹 교차 검증으로 성능 측정 / n_jobs 코어 개수 4개 \n",
    "end = time.time()  \n",
    "print(\"하이퍼 매개변수 최적화에 걸린 시간은\", end-start, \"초 입니다.\")\n",
    "\n",
    "# 교차 검증 결과의 평균과 분산 구하기\n",
    "train_mean = np.mean(train_score, axis=1)\n",
    "train_std = np.std(train_score, axis=1)\n",
    "test_mean = np.mean(test_score, axis=1)\n",
    "test_std = np.std(test_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.96009865 0.96378505 0.96750606 0.96748875 0.97493077 0.96843198\n",
      " 0.97213569 0.97120976 0.97028384 0.97307892 0.97306161 0.97398754\n",
      " 0.97214434 0.97863448 0.97308757 0.97492212 0.97400485 0.97491346\n",
      " 0.97397889 0.97307027]\n",
      "[0.02278838 0.01891113 0.02009961 0.01968894 0.01615651 0.02004213\n",
      " 0.0161779  0.01472664 0.01606159 0.01838021 0.01885033 0.01855666\n",
      " 0.0132053  0.01389095 0.01278473 0.01672838 0.01752405 0.01509315\n",
      " 0.01994379 0.01412044]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5e9acfa9e81c6dc9dade1ddfb0eacd10fb82856ca55f7e1427b6e32fc559d96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
